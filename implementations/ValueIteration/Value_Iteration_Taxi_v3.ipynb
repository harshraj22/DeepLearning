{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Value_Iteration_Taxi_v3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83n4ptwCZF8P"
      },
      "source": [
        "## Trying Value iteration on Taxi Game Environment from Openai\n",
        "- Following [this](https://www.kaggle.com/charel/learn-by-example-reinforcement-learning-with-gym#Basic-Q-learning-algorithm) tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iInXj1wsdcG1"
      },
      "source": [
        "#### Explore Taxi-v3 Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmOk0fGNY79i"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from math import inf\n",
        "from tqdm import tqdm\n",
        "\n",
        "# List all available environments in openai-gym\n",
        "# print(*gym.envs.registry.all(), sep='\\n')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dwhqpU_ZEu7"
      },
      "source": [
        "# Details about Taxi-v3: https://gym.openai.com/envs/Taxi-v3/\n",
        "\n",
        "env = gym.make('Taxi-v3')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiKGWZGFca6d",
        "outputId": "d4dde4bf-a0c5-403d-a0c2-8f31300840b2"
      },
      "source": [
        "N_STATES, N_ACTIONS = env.observation_space.n, env.action_space.n\n",
        "print(f'Taxi-v3 environment with {N_STATES} states, and {N_ACTIONS} actions')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taxi-v3 environment with 500 states, and 6 actions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHjCOmIdauIF",
        "outputId": "742567a1-add8-4a92-d2d0-06ea9d6779d1"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YjqTFSTdh9Y"
      },
      "source": [
        "### Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wscu2nKE87n-"
      },
      "source": [
        "# if value of any state does not improve more than SIGNIFICAN_IMPROVEMENT\n",
        "# stop updating\n",
        "SIGNIFICANT_IMPROVEMENT = 0.01 \n",
        "\n",
        "# the discount factor\n",
        "GAMMA = 0.9"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA3JEdTMcBQW"
      },
      "source": [
        "def choose_best_action(env, V, cur_state):\n",
        "  \"\"\"Given a state, and Values, it returns the action\n",
        "  which corresponds to maximum future reward\"\"\"\n",
        "\n",
        "  best_reward, best_action = -inf, None\n",
        "\n",
        "  for action in range(N_ACTIONS):\n",
        "    env.env.s = cur_state\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "\n",
        "    # consider the future discounted reward to decide the\n",
        "    # best action to take from the given state\n",
        "    reward = reward + GAMMA * V[next_state]\n",
        "\n",
        "    if reward > best_reward:\n",
        "      best_reward, best_action = reward, action\n",
        "    \n",
        "  env.env.s = cur_state\n",
        "  return best_action"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox6kUa9ycBLp",
        "outputId": "6230cf56-9db9-4ed9-ab0d-06d4ac27e906"
      },
      "source": [
        "V = np.zeros(N_STATES)\n",
        "pbar, iterations = tqdm(), 0\n",
        "\n",
        "while True:\n",
        "  # play games and update values till there is\n",
        "  # no significant improvement\n",
        "  improvement = 0\n",
        "  env.reset()\n",
        "  total_reward = 0\n",
        "\n",
        "  for cur_state in range(N_STATES):\n",
        "    next_action = choose_best_action(env, V, cur_state)\n",
        "\n",
        "    next_state, reward, done, info = env.step(next_action)\n",
        "    \n",
        "    # calculate the improvement in value\n",
        "    improvement = max(improvement, abs(V[cur_state]-(reward + GAMMA * V[next_state])))\n",
        "\n",
        "    # update the current state's value\n",
        "    V[cur_state] = reward + GAMMA * V[next_state]\n",
        "    total_reward += reward\n",
        "\n",
        "  # if there is no significant improvement in value over all the\n",
        "  #  states, stop the value iteration\n",
        "  if improvement < SIGNIFICANT_IMPROVEMENT:# or iterations > 5:\n",
        "    break\n",
        "\n",
        "  iterations += 1\n",
        "  pbar.update(iterations)\n",
        "  pbar.set_postfix({'improvement': f'{improvement:.3f}', 'reward': total_reward, 'iterations': iterations})\n",
        "\n",
        "pbar.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "820it [00:03, 269.54it/s, improvement=0.012, reward=-416, iterations=40]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RCXdmnlcBJB"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK2Ue-bFnbAQ"
      },
      "source": [
        "### Let us play the game using the value function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YvsMYJ9cBGb",
        "outputId": "a8556662-f373-46dd-f8cd-89a5acceeb54"
      },
      "source": [
        "cur_state = env.reset()\n",
        "done, total_reward = False, 0\n",
        "reward = 0\n",
        "\n",
        "while not done:\n",
        "  next_action = choose_best_action(env, V, cur_state)\n",
        "\n",
        "  cur_state, reward, done, info = env.step(next_action)\n",
        "  env.render()\n",
        "  print(f'Reward: {reward}')\n",
        "\n",
        "  total_reward += reward\n",
        "\n",
        "print(f'Total Reward: {total_reward} | Done: {done}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B:\u001b[42m_\u001b[0m|\n",
            "+---------+\n",
            "  (East)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[42m_\u001b[0m|\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : :\u001b[42m_\u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : :\u001b[42m_\u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Reward: -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Reward: 20\n",
            "Total Reward: 7 | Done: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chjsqGYS4f0i"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}